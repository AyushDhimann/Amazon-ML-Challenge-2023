{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhIDBMSN3ucZp0KGNEqSPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyushDhimann/Amazon-ML-Challenge-2023/blob/main/Team_D_N_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "By Team D.N.A\n",
        "\n",
        "AYUSH DHIMAN - https://github.com/AyushDhimann , https://www.linkedin.com/in/ayushdhimann/\n",
        "ARCHIT GARG - https://github.com/Architgarg2003 , https://www.linkedin.com/in/architgarg2003/\n",
        "NIDA FATIMA - https://github.com/Nida-18 , https://www.linkedin.com/in/NidaFatimaAlam/\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "no9hB6Xn1kGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3-ap-southeast-1.amazonaws.com/he-public-data/datasetb2d9982.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKOcDQpMj7cx",
        "outputId": "dd4550f7-6106-473c-f866-7ad5cfabd1b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-24 04:40:18--  https://s3-ap-southeast-1.amazonaws.com/he-public-data/datasetb2d9982.zip\n",
            "Resolving s3-ap-southeast-1.amazonaws.com (s3-ap-southeast-1.amazonaws.com)... 52.219.124.82\n",
            "Connecting to s3-ap-southeast-1.amazonaws.com (s3-ap-southeast-1.amazonaws.com)|52.219.124.82|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 895569552 (854M) [binary/octet-stream]\n",
            "Saving to: ‘datasetb2d9982.zip’\n",
            "\n",
            "datasetb2d9982.zip  100%[===================>] 854.08M  13.8MB/s    in 64s     \n",
            "\n",
            "2023-04-24 04:41:23 (13.3 MB/s) - ‘datasetb2d9982.zip’ saved [895569552/895569552]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip datasetb2d9982.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0T3lwl7kDOk",
        "outputId": "0c0cb079-0405-4052-aa4e-695bb0b4a348"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  datasetb2d9982.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/sample_submission.csv  \n",
            "  inflating: dataset/train.csv       \n",
            "  inflating: dataset/test.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir model prediction sorted"
      ],
      "metadata": {
        "id": "vPEDrgMmBWK9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SORTING TRAIN.CSV\n",
        "\n",
        "\n",
        "import csv\n",
        "\n",
        "input_file = \"dataset/train.csv\"\n",
        "output_file = \"sorted/sortedtrain.csv\"\n",
        "sort_column = \"PRODUCT_TYPE_ID\"\n",
        "\n",
        "# Read the CSV file and sort the rows based on the specified column\n",
        "with open(input_file, 'r') as csv_file:\n",
        "    reader = csv.DictReader(csv_file)\n",
        "    sorted_rows = sorted(reader, key=lambda row: float(row[sort_column]))\n",
        "\n",
        "# Write the sorted rows to a new CSV file\n",
        "with open(output_file, 'w', newline='') as csv_file:\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=reader.fieldnames)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(sorted_rows)\n",
        "\n",
        "print(\"\\033[0;32mDONE\\033[0;32m\")"
      ],
      "metadata": {
        "id": "IESpZoRtxPG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d6d5405-dd3e-4de6-c225-4c879bd9a86b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;32mDONE\u001b[0;32m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SORTING TEST.CSV\n",
        "\n",
        "\n",
        "import csv\n",
        "\n",
        "input_file = \"dataset/test.csv\"\n",
        "output_file = \"sorted/sortedtest.csv\"\n",
        "sort_column = \"PRODUCT_TYPE_ID\"\n",
        "\n",
        "# Read the CSV file and sort the rows based on the specified column\n",
        "with open(input_file, 'r') as csv_file:\n",
        "    reader = csv.DictReader(csv_file)\n",
        "    sorted_rows = sorted(reader, key=lambda row: float(row[sort_column]))\n",
        "\n",
        "# Write the sorted rows to a new CSV file\n",
        "with open(output_file, 'w', newline='') as csv_file:\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=reader.fieldnames)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(sorted_rows)\n",
        "\n",
        "print(\"\\033[0;32mDONE\\033[0;32m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qktx9DY7kSs9",
        "outputId": "2f2c9029-a1e3-4d5a-b50b-34fd41429905"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;32mDONE\u001b[0;32m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CUTTING INTO UNIQUE PRODUCT_TYPE_ID\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "df1 = pd.read_csv('sorted/sortedtrain.csv')\n",
        "df2 = pd.read_csv('sorted/sortedtest.csv')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "if not os.path.exists('train'):\n",
        "    os.makedirs('train')\n",
        "if not os.path.exists('test'):\n",
        "    os.makedirs('test')\n",
        "\n",
        "# Split the data into train and test sets based on PRODUCT_TYPE_ID\n",
        "for id in df1['PRODUCT_TYPE_ID'].unique():\n",
        "    id_df = df1[df1['PRODUCT_TYPE_ID'] == id]\n",
        "    id_df.to_csv(f'train/{id}.csv', index=False)\n",
        "\n",
        "for id in df2['PRODUCT_TYPE_ID'].unique():\n",
        "    id_df = df2[df2['PRODUCT_TYPE_ID'] == id]\n",
        "    id_df.to_csv(f'test/{id}.csv', index=False)\n",
        "\n",
        "print(\"\\033[0;32mDONE\\033[0;32m\")"
      ],
      "metadata": {
        "id": "GI5B7-PCx1Oa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e4608a-4bf6-4b33-c4b2-c33f29e77105"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;32mDONE\u001b[0;32m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CUTTING FILES FROM TRAIN/TEST CSV FILE FOR THE RANGE 2100-2199\n",
        "\n",
        "import csv\n",
        "\n",
        "start_row = 299210  # The first row to keep\n",
        "end_row = 301066  # The last row to keep\n",
        "\n",
        "with open('sorted/', 'r') as input_file, open('micro.csv', 'w', newline='') as output_file:\n",
        "    reader = csv.reader(input_file)\n",
        "    writer = csv.writer(output_file)\n",
        "    row_num = 1\n",
        "    for row in reader:\n",
        "        if row_num >= start_row and row_num <= end_row:\n",
        "            writer.writerow(row)\n",
        "        elif row_num > end_row:\n",
        "            break\n",
        "        row_num += 1\n"
      ],
      "metadata": {
        "id": "NuAAas54Dsfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ADDING THE ESSENTIAL LINE ON TOP OF MICRO.CSV\n",
        "\n",
        "with open('micro.csv', 'r') as input_file, open('mini.csv', 'w', newline='') as output_file:\n",
        "    # Create a new writer to write to the output file\n",
        "    writer = csv.writer(output_file)\n",
        "\n",
        "    # Add the new line at the beginning of the file\n",
        "    writer.writerow(['PRODUCT_ID', 'TITLE', 'BULLET_POINTS','DESCRIPTION','PRODUCT_TYPE_ID','PRODUCT_LENGTH'])\n",
        "\n",
        "    # Write the remaining lines from the input file to the output file\n",
        "    reader = csv.reader(input_file)\n",
        "    for row in reader:\n",
        "        writer.writerow(row)\n"
      ],
      "metadata": {
        "id": "oWpMpSo3EGbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find train -type f | wc -l\n",
        "!find test -type f | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XJl5H3IAvZj",
        "outputId": "708ccb2e-f754-45ee-8b22-8440e3d42270"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12907\n",
            "10565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CUTTING THE GIVEN CSV FILE INTO FIRST N LINES\n",
        "\n",
        "import csv\n",
        "\n",
        "# Define the input and output file paths\n",
        "input_file = \"train/megatest.csv\"\n",
        "output_file = 'cut.csv'\n",
        "\n",
        "# Define the number of lines to cut the file\n",
        "num_lines = 1000\n",
        "\n",
        "# Open the input and output files\n",
        "with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
        "    # Create a CSV reader object\n",
        "    reader = csv.reader(infile)\n",
        "\n",
        "    # Create a CSV writer object\n",
        "    writer = csv.writer(outfile)\n",
        "\n",
        "    # Write the first `num_lines` lines to the output file\n",
        "    for i, row in enumerate(reader):\n",
        "        if i >= num_lines:\n",
        "            break\n",
        "        writer.writerow(row)\n",
        "\n",
        "\n",
        "print(\"\\033[0;32mDONE\\033[0;32m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9emJzIxLmPcr",
        "outputId": "e9de7df0-b8df-41c3-fc6d-2153b1f57198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MERGING TWO CSV FILES FROM TRAIN AND TEST ALTOGETHER\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the first CSV file\n",
        "df1 = pd.read_csv('train/2213.csv')\n",
        "df11 = pd.read_csv('test/2213.csv')\n",
        "\n",
        "# Load the second CSV file\n",
        "df2 = pd.read_csv('train/5937.csv')\n",
        "df22 = pd.read_csv('test/5937.csv')\n",
        "\n",
        "# Merge the two dataframes\n",
        "merged_df1 = pd.concat([df1, df2])\n",
        "merged_df2 = pd.concat([df11, df22])\n",
        "\n",
        "# Write the merged dataframe to a new CSV file\n",
        "merged_df1.to_csv('train/123456.csv', index=False)\n",
        "merged_df2.to_csv('test/123456.csv', index=False)\n",
        "print(\"\\033[0;32mDONE\\033[0;32m\")"
      ],
      "metadata": {
        "id": "jZ33V_jr0hIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RANDOMLY MERGING GIVEN NUMBER OF FILES FROM TRAIN AND TEST\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Define the number of files to select randomly\n",
        "num_files = 5\n",
        "\n",
        "# Randomly select files in the range of 1000 and 5000\n",
        "file_ids = random.sample(range(1000, 9000), num_files)\n",
        "\n",
        "# Load the CSV files\n",
        "df_list = []\n",
        "for file_id in file_ids:\n",
        "    train_file = f'train/{file_id}.csv'\n",
        "    test_file = f'test/{file_id}.csv'\n",
        "    if os.path.isfile(train_file) and os.path.isfile(test_file):\n",
        "        df_train = pd.read_csv(train_file)\n",
        "        df_test = pd.read_csv(test_file)\n",
        "        df_list.append(df_train)\n",
        "        df_list.append(df_test)\n",
        "    else:\n",
        "        print(f\"File {train_file} or {test_file} not found. Finding another random file.\")\n",
        "        continue\n",
        "\n",
        "# Merge the dataframes\n",
        "merged_train_df = pd.concat(df_list[::2])\n",
        "merged_test_df = pd.concat(df_list[1::2])\n",
        "\n",
        "# Write the merged dataframes to new CSV files\n",
        "merged_train_df.to_csv('train/1234560.csv', index=False)\n",
        "merged_test_df.to_csv('test/1234560.csv', index=False)\n",
        "\n",
        "print(\"\\033[0;32mDONE\\033[0;32m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HZfVQ53AbcO",
        "outputId": "4c7647a9-48ba-432d-c2a5-d72addd10b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File train/5874.csv or test/5874.csv not found. Finding another random file.\n",
            "\u001b[0;32mDONE\u001b[0;32m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTING ESSESNTIAL LIBRARIES\n",
        "\n",
        "import os.path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, precision_score, f1_score, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import xgboost as xgb\n",
        "print(\"\\033[0;32mDONE\\033[0;32m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpER7MKe6L3G",
        "outputId": "5e5defe0-bdc1-4956-aeca-a6756ed0d580"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;32mDONE\u001b[0;32m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvVSNCpBnTub",
        "outputId": "113976ff-ae24-4227-f0b5-96cc7405a625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTING THE FILE\n",
        "\n",
        "#randomfile = random.randint(1000, 9000)\n",
        "randomfile = 2213\n",
        "# print(randomfile)\n",
        "\n",
        "filename = f\"train/{randomfile}.csv\"\n",
        "#filename = \"train/2213.csv\"\n",
        "\n",
        "train_df = pd.read_csv(filename)\n",
        "print(\"\\033[0;32mDONE\\033[0;32m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lchfHQkL6ODn",
        "outputId": "bbccacb0-0975-49c1-a5ad-b2c7a9ea225d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;32mDONE\u001b[0;32m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (AIO) XGBOOST MODEL THAT USES GPU FOR TRAINING AND PREDICTION WHICH GIVES THE ACCURACY AND SCORE AS OUTPUT\n",
        "\n",
        "\n",
        "# preprocess the text\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "    return \" \".join(stemmed_tokens)\n",
        "\n",
        "train_df[\"preprocessed_text\"] = train_df[\"TITLE\"].fillna(\"\") + \" \" + train_df[\"DESCRIPTION\"].fillna(\"\") + \" \" + train_df[\"BULLET_POINTS\"].fillna(\"\")\n",
        "train_df[\"preprocessed_text\"] = train_df[\"preprocessed_text\"].apply(preprocess_text)\n",
        "\n",
        "# create a TF-IDF representation\n",
        "vectorizer = HashingVectorizer(n_features=20000, ngram_range=(1, 3), norm=None)\n",
        "tfidf_matrix = vectorizer.fit_transform(train_df[\"preprocessed_text\"])\n",
        "\n",
        "# split the data into training and validation sets\n",
        "y = train_df[\"PRODUCT_LENGTH\"].values\n",
        "X_train, X_val, y_train, y_val = train_test_split(tfidf_matrix, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# create an XGBoost DMatrix for training and validation\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "# specify GPU training parameters\n",
        "gpu_params = {\"tree_method\": \"gpu_hist\", \"gpu_id\": 0}\n",
        "\n",
        "# train an XGBoost regression model with early stopping\n",
        "model = xgb.train(gpu_params, dtrain, num_boost_round=1000, evals=[(dval, \"validation\")], early_stopping_rounds=5, verbose_eval=False)\n",
        "test_df = pd.read_csv(f\"test/{randomfile}.csv\")\n",
        "test_df[\"preprocessed_text\"] = test_df[\"TITLE\"].fillna(\"\") + \" \" + test_df[\"DESCRIPTION\"].fillna(\"\") + \" \" + test_df[\"BULLET_POINTS\"].fillna(\"\")\n",
        "test_df[\"preprocessed_text\"] = test_df[\"preprocessed_text\"].apply(preprocess_text)\n",
        "test_tfidf_matrix = vectorizer.transform(test_df[\"preprocessed_text\"])\n",
        "dtest = xgb.DMatrix(test_tfidf_matrix)\n",
        "test_df[\"PRODUCT_LENGTH_PREDICTED\"] = model.predict(dtest)[:len(test_df)]\n",
        "\n",
        "# create a CSV file containing the predicted values and their respective product IDs\n",
        "output_df = test_df[[\"PRODUCT_ID\", \"PRODUCT_LENGTH_PREDICTED\"]]\n",
        "output_df.to_csv(\"prediction/predicted_lengthsAIO.csv\", index=False)\n",
        "\n",
        "# evaluate the model on the validation set\n",
        "y_val_pred = model.predict(dval)\n",
        "mae = mean_absolute_error(y_val, y_val_pred)\n",
        "maep = mean_absolute_percentage_error(y_val, y_val_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "r_squared = r2_score(y_val, y_val_pred)\n",
        "accuracy = r_squared * 100.0\n",
        "score = max( 0 , 100*(1-float(maep)))\n",
        "print(\"Validation MAE:\", mae)\n",
        "print(\"Validation MAE percentage:\", maep)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy))\n",
        "print()\n",
        "print(\"\\033[0;32mScore is : \\033[0;32m\",score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USjLeDwTt6yL",
        "outputId": "8c7e709f-10b8-403d-cc4a-080b1c740fb9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MAE: 10.857432401413535\n",
            "Validation MAE percentage: 0.029436954269618443\n",
            "RMSE: 52.31490099090869\n",
            "Accuracy: 94.75%\n",
            "\n",
            "\u001b[0;32mScore is : \u001b[0;32m 97.05630457303815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SOME SAVED TEST VALUES\n",
        "\n",
        "# 1534 - Validation MAE: 977.1455688476562 Accuracy: -462.99%\n",
        "# 1860 - Validation MAE: 355.4024206390624 Accuracy: 24.45%\n",
        "# 2213 - Validation MAE: 9.014890544202304 Accuracy: 98.91%\n",
        "# 3946 - Validation MAE: 11.690439372813508 Validation MAE percentage: 0.03647051083007642 RMSE: 56.33427523510723 Accuracy: 93.91% Score is :  96.35294891699235"
      ],
      "metadata": {
        "id": "p-mviVO039MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBOOST MODEL TO ONLY TRAIN AND SAVE THE TRAINED MODEL\n",
        "\n",
        "\n",
        "# preprocess the text\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "    return \" \".join(stemmed_tokens)\n",
        "\n",
        "train_df[\"preprocessed_text\"] = train_df[\"TITLE\"].fillna(\"\") + \" \" + train_df[\"DESCRIPTION\"].fillna(\"\") + \" \" + train_df[\"BULLET_POINTS\"].fillna(\"\")\n",
        "train_df[\"preprocessed_text\"] = train_df[\"preprocessed_text\"].apply(preprocess_text)\n",
        "\n",
        "# create a TF-IDF representation\n",
        "vectorizer = HashingVectorizer(n_features=20000, ngram_range=(1, 2), norm=None)\n",
        "\n",
        "# split the data into training and validation sets\n",
        "y = train_df[\"PRODUCT_LENGTH\"].values\n",
        "\n",
        "# set the chunk size for training\n",
        "chunk_size = 100\n",
        "\n",
        "# check if the model file exists, if so, load the existing model\n",
        "model_file = \"model/trained_model.model\"\n",
        "existing_model = None  # Define the variable outside the if statement\n",
        "if os.path.isfile(model_file):\n",
        "    existing_model = xgb.Booster(model_file=model_file)\n",
        "    models = [existing_model]\n",
        "else:\n",
        "    models = []\n",
        "\n",
        "# loop over the data by chunks and train the model\n",
        "for i in tqdm(range(0, len(train_df), chunk_size)):\n",
        "    df_chunk = train_df.iloc[i:i+chunk_size]\n",
        "    tfidf_matrix = vectorizer.fit_transform(df_chunk[\"preprocessed_text\"])\n",
        "    X_train, X_val, y_train, y_val = train_test_split(tfidf_matrix, y[i:i+chunk_size], test_size=0.2, random_state=42)\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dval = xgb.DMatrix(X_val, label=y_val)\n",
        "    gpu_params = {\"tree_method\": \"gpu_hist\", \"gpu_id\": 0}\n",
        "    model = xgb.train(gpu_params, dtrain, num_boost_round=1000, evals=[(dval, \"validation\")], early_stopping_rounds=5, verbose_eval=False)\n",
        "    # append the new model to the existing list of models\n",
        "    models.append(model)\n",
        "\n",
        "# train the final model by concatenating all the models together\n",
        "final_model = xgb.Booster(model_file=None)\n",
        "final_model = xgb.train({}, dtrain, num_boost_round=0, xgb_model=existing_model, verbose_eval=False)\n",
        "for m in models:\n",
        "    final_model = xgb.train({}, dtrain, num_boost_round=0, xgb_model=m, verbose_eval=False)\n",
        "final_model.save_model(f\"model/trained_model.model\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"\\033[0;32mTraining model saved.\\033[0;32m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wi5j4Iqi2O0",
        "outputId": "3a24dae2-b370-496d-974b-d09c6525486f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "100%|██████████| 28/28 [00:11<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\u001b[0;32mTraining model saved.\u001b[0;32m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING THE SAVED TRAINED MODEL AND MAKING FUTURE PREDECTIONS\n",
        "\n",
        "# load the trained model\n",
        "trained_models = []\n",
        "print(\"Loading Training Model\")\n",
        "for i in range(len(train_df) // chunk_size):\n",
        "    trained_models.append(xgb.Booster())\n",
        "    trained_models[i].load_model(f\"model/trained_model.model\")\n",
        "\n",
        "# concatenate the predicted values for all the chunks\n",
        "test_df = pd.read_csv(f\"test/{randomfile}.csv\")\n",
        "test_df[\"preprocessed_text\"] = test_df[\"TITLE\"].fillna(\"\") + \" \" + test_df[\"DESCRIPTION\"].fillna(\"\") + \" \" + test_df[\"BULLET_POINTS\"].fillna(\"\")\n",
        "test_df[\"preprocessed_text\"] = test_df[\"preprocessed_text\"].apply(preprocess_text)\n",
        "test_tfidf_matrix = vectorizer.transform(test_df[\"preprocessed_text\"])\n",
        "\n",
        "# predict using the saved models\n",
        "predictions = []\n",
        "for i in range(len(train_df) // chunk_size):\n",
        "    dtest = xgb.DMatrix(test_tfidf_matrix[i*chunk_size:(i+1)*chunk_size])\n",
        "    prediction_chunk = trained_models[i].predict(dtest)\n",
        "    predictions.extend(prediction_chunk)\n",
        "\n",
        "# predict the remaining data that is not divisible by chunk_size\n",
        "if len(test_df) % chunk_size != 0:\n",
        "    dtest = xgb.DMatrix(test_tfidf_matrix[(len(train_df) // chunk_size)*chunk_size:])\n",
        "    prediction_chunk = trained_models[-1].predict(dtest)\n",
        "    predictions.extend(prediction_chunk)\n",
        "\n",
        "# add the predicted values to the test_df and save to CSV\n",
        "test_df[\"PRODUCT_LENGTH_PREDICTED\"] = predictions\n",
        "output_df = test_df[[\"PRODUCT_ID\", \"PRODUCT_LENGTH_PREDICTED\"]]\n",
        "output_df.to_csv(\"prediction/predicted_lengths.csv\", index=False)\n",
        "print(\"\\n\\n\")\n",
        "print(\"\\033[0;32mPrediction Model Saved\\033[0;32m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpTWprqSq9DS",
        "outputId": "911bfa2a-ba45-4ec3-e3c9-64eb895e11c8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Training Model\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "[05:07:59] WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0\n",
            "\n",
            "\n",
            "\n",
            "\u001b[0;32mPrediction Model Saved\u001b[0;32m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING USING XGBOOST BY USING ALL THE FILES IN /train DIRECTORY\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "    return \" \".join(stemmed_tokens)\n",
        "\n",
        "# set the chunk size for training\n",
        "chunk_size = 100\n",
        "\n",
        "# check if the model file exists, if so, load the existing model\n",
        "model_file = \"model/trained_model.model\"\n",
        "existing_model = None\n",
        "if os.path.isfile(model_file):\n",
        "    existing_model = xgb.Booster(model_file=model_file)\n",
        "    models = [existing_model]\n",
        "else:\n",
        "    models = []\n",
        "\n",
        "# loop over train files based on the product type ID\n",
        "for file_name in os.listdir(\"train\"):\n",
        "    file_path = os.path.join(\"train\", file_name)\n",
        "    print(f\"Training on {file_path}\")\n",
        "    \n",
        "    df_chunk = pd.read_csv(file_path)\n",
        "    df_chunk[\"preprocessed_text\"] = df_chunk[\"TITLE\"].fillna(\"\") + \" \" + df_chunk[\"DESCRIPTION\"].fillna(\"\") + \" \" + df_chunk[\"BULLET_POINTS\"].fillna(\"\")\n",
        "    df_chunk[\"preprocessed_text\"] = df_chunk[\"preprocessed_text\"].apply(preprocess_text)\n",
        "\n",
        "    # create a TF-IDF representation\n",
        "    vectorizer = HashingVectorizer(n_features=20000, ngram_range=(1, 2), norm=None)\n",
        "    tfidf_matrix = vectorizer.fit_transform(df_chunk[\"preprocessed_text\"])\n",
        "    if tfidf_matrix.shape[0] < 2:  # check that there are at least two samples\n",
        "        continue\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(tfidf_matrix, df_chunk[\"PRODUCT_LENGTH\"].values, test_size=0.2, random_state=42)\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dval = xgb.DMatrix(X_val, label=y_val)\n",
        "    model = xgb.train({}, dtrain, num_boost_round=1000, evals=[(dval, \"validation\")], early_stopping_rounds=5, verbose_eval=False)\n",
        "    # append the new model to the existing list of models\n",
        "    models.append(model)\n",
        "\n",
        "# train the final model by concatenating all the models together\n",
        "dtrain_full = xgb.DMatrix(tfidf_matrix, label=df_chunk[\"PRODUCT_LENGTH\"].values)\n",
        "final_model = xgb.Booster(model_file=None)\n",
        "final_model = xgb.train({}, dtrain_full, num_boost_round=0, xgb_model=existing_model, verbose_eval=False)\n",
        "for m in models:\n",
        "    final_model = xgb.train({}, dtrain_full, num_boost_round=0, xgb_model=m, verbose_eval=False)\n",
        "final_model.save_model(f\"model/trained_model.model\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"\\033[0;32mTraining model saved.\\033[0;32m\")"
      ],
      "metadata": {
        "id": "Tuqz6up2DGwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICTING ON THE SAVED MODEL USED TO TRAIN /train\n",
        "\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the pre-trained XGBoost model\n",
        "print(\"Loading pre-trained model...\")\n",
        "bst = xgb.Booster()\n",
        "bst.load_model(\"model/trained_model_.model\")\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# Load the file to make predictions on\n",
        "test_df = pd.read_csv(\"puzzie.csv\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "    return \" \".join(stemmed_tokens)\n",
        "\n",
        "\n",
        "# Preprocess the text data in the file\n",
        "test_df[\"preprocessed_text\"] = test_df[\"TITLE\"].fillna(\"\") + \" \" + test_df[\"DESCRIPTION\"].fillna(\"\") + \" \" + test_df[\"BULLET_POINTS\"].fillna(\"\")\n",
        "test_df[\"preprocessed_text\"] = test_df[\"preprocessed_text\"].apply(preprocess_text)\n",
        "\n",
        "# Convert the preprocessed text into a TF-IDF matrix\n",
        "vectorizer = HashingVectorizer(n_features=10000, ngram_range=(1, 2), norm=None)\n",
        "vectorizer.fit(test_df[\"preprocessed_text\"])\n",
        "\n",
        "test_tfidf_matrix = vectorizer.transform(test_df[\"preprocessed_text\"])\n",
        "\n",
        "# Make predictions using the pre-trained model\n",
        "dtest = xgb.DMatrix(test_tfidf_matrix)\n",
        "predictions = bst.predict(dtest)\n",
        "\n",
        "# Add the predicted values to the test_df and save to CSV\n",
        "test_df[\"PRODUCT_LENGTH_PREDICTED\"] = predictions\n",
        "output_df = test_df[[\"PRODUCT_ID\", \"PRODUCT_LENGTH_PREDICTED\"]]\n",
        "output_df.to_csv(\"prediction/predicted_lengths.csv\", index=False)\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"\\033[0;32mPrediction Model Saved\\033[0;32m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nZpO4RqIcux",
        "outputId": "81d5dd0b-a368-48c9-a38d-636165c3c5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-trained model...\n",
            "Model loaded.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[0;32mPrediction Model Saved\u001b[0;32m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FINDING VALUES INSIDE SORTEDTEST.CSV, CALCULATING AND DIRECTLY SAVING THEM TO predicted_lengths.csv ALONGSIDE THE TRAINED MODEL FOR HIGHER ACCURACY AT THE END\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import xgboost as xgb\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# Define a compiled regular expression pattern for matching the length information\n",
        "length_pattern = re.compile(r'\\b(\\d+(\\.\\d+)?)\\s*(in(ch(es)?)?|\"|\\'\\'|mm|cm|ft|feet|inches|Inch|in)\\b', re.IGNORECASE)\n",
        "\n",
        "# Define a function to extract the maximum length from a description\n",
        "def extract_max_length(description):\n",
        "    # Use regular expression to extract length information from the description\n",
        "    matches = length_pattern.findall(description)\n",
        "\n",
        "    # If matches are found, extract the length values and units\n",
        "    if matches:\n",
        "        length_values = []\n",
        "        for match in matches:\n",
        "            length_value = float(match[0]) if match[0] else float(match[2])\n",
        "            length_unit = match[1] if match[1] else match[3]\n",
        "\n",
        "            # Convert the length to a standard unit (e.g. inches)\n",
        "            if length_unit.lower() == 'mm':\n",
        "                length_value = length_value / 25.4\n",
        "            elif length_unit.lower() == 'cm':\n",
        "                length_value = length_value / 2.54\n",
        "            elif length_unit.lower() in ['ft', 'feet', '\\'']:\n",
        "                length_value = length_value * 12\n",
        "\n",
        "            length_values.append(length_value)\n",
        "\n",
        "        # Output the maximum length value and unit\n",
        "        max_length_value = round(max(length_values), 2)\n",
        "        return max_length_value\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Load the CSV file using pandas\n",
        "df = pd.read_csv('test.csv', encoding='iso-8859-1')\n",
        "\n",
        "# Combine the columns 'TITLE', 'BULLET_POINTS', and 'DESCRIPTION' into a single 'description' column\n",
        "df['description'] = df['TITLE'].fillna('') + ' ' + df['BULLET_POINTS'].fillna('') + ' ' + df['DESCRIPTION'].fillna('')\n",
        "\n",
        "# Extract the maximum length for each row using the 'extract_max_length' function\n",
        "df['max_length'] = df['description'].apply(extract_max_length)\n",
        "\n",
        "# Load the pre-trained XGBoost model\n",
        "print(\"Loading pre-trained model...\")\n",
        "bst = xgb.Booster()\n",
        "bst.load_model(\"model/trained_model_.model\")\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# Load the file to make predictions on\n",
        "test_df = pd.read_csv(\"puzzie.csv\")\n",
        "\n",
        "# Preprocess the text data in the file\n",
        "stop_words = stopwords.words('english')\n",
        "stemmer = SnowballStemmer('english')\n",
        "test_df[\"preprocessed_text\"] = test_df[\"TITLE\"].fillna(\"\") + \" \" + test_df[\"DESCRIPTION\"].fillna(\"\") + \" \" + test_df[\"BULLET_POINTS\"].fillna(\"\")\n",
        "test_df[\"preprocessed_text\"] = test_df[\"preprocessed_text\"].apply(lambda x: preprocess_text(x, stop_words, stemmer))\n",
        "\n",
        "# Convert the preprocessed text into a TF-IDF matrix\n",
        "vectorizer = HashingVectorizer(n_features=10000, ngram_range=(1, 2), norm=None)\n",
        "vectorizer.fit(test_df[\"preprocessed_text\"])\n",
        "\n",
        "test_tfidf_matrix = vectorizer.transform(test_df[\"preprocessed_text\"])\n",
        "\n",
        "# Make predictions using the pre-trained model\n",
        "dtest = xgb.DMatrix(test_tfidf_matrix)\n",
        "predictions = bst.predict(dtest)\n",
        "\n",
        "# Add the predicted values to the test_df and save to CSV\n",
        "test_df[\"PRODUCT_LENGTH_PREDICTED\"] = predictions\n",
        "output_df = test_df[[\"PRODUCT_ID\", \"PRODUCT_LENGTH_PREDICTED\"]]\n",
        "\n"
      ],
      "metadata": {
        "id": "BsjHfIQHRAqM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
